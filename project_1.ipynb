{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project 1 ",
      "provenance": [],
      "collapsed_sections": [
        "g7iQgn7x4Ju1",
        "_e7nFMOD0Zso",
        "-Jpg4HVN1BeK",
        "FFZ2yWxc4LKR",
        "ZE6bgv6yv5p2",
        "OEwMOS8V0WzX",
        "xAECpBrhcNlI",
        "KCHb8_GeVeu2",
        "I5Wz4TL8VuiM",
        "pawNvEqc1lih",
        "5vZ6k5vW2u5u"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pwliuab/FaceMaskDetection/blob/main/project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vMZjsgQ4CtO"
      },
      "source": [
        "# COMP4211 Project 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7iQgn7x4Ju1"
      },
      "source": [
        "## Basic setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-A5pApotJaM",
        "outputId": "63e02329-46aa-49ce-cab9-a180d516e88b"
      },
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiN3qM-ilgcP"
      },
      "source": [
        "# unzip your dataset, [user] may specifies their own directories\n",
        "!unzip gdrive/MyDrive/pa3/archive.zip -d gdrive/MyDrive/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rlOeDMkbDqT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV9y6i0e0W58"
      },
      "source": [
        "## Helper Functions and Global Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e7nFMOD0Zso"
      },
      "source": [
        "### Define function to convert label from words and numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLniULcF0mcp"
      },
      "source": [
        "def label_to_num(label):\n",
        "  if label == 'with_mask':\n",
        "    return 0\n",
        "  elif label == 'without_mask':\n",
        "    return 1\n",
        "  else:\n",
        "    return 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jpg4HVN1BeK"
      },
      "source": [
        "### List storing all labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFlN_vmy1Ghj"
      },
      "source": [
        "ALL_LABELS = ['with_mask', 'without_mask', 'mask_weared_incorrect']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFZ2yWxc4LKR"
      },
      "source": [
        "### Define function for save and load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw05hRS34RVV"
      },
      "source": [
        "# Define save function\n",
        "import torch.nn as nn\n",
        "def save(path, model, optimizer, validation_accuracy, validation_loss):\n",
        "  save_path = path\n",
        "  state_dict = {'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'validation_accuracy': validation_accuracy,\n",
        "              'validation_loss': validation_loss}\n",
        "\n",
        "  torch.save(state_dict, save_path)\n",
        "  print(f'Model saved to {save_path}')\n",
        "\n",
        "# Define load function\n",
        "# Result returned is a tuple stroing (validation_accuracy, validation_loss)\n",
        "def load(path, model, optimizer):\n",
        "  load_path = path \n",
        "  state_dict = torch.load(load_path)\n",
        "  model.load_state_dict(state_dict['model_state_dict'])\n",
        "  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "  validation_accuracy = state_dict['validation_accuracy']\n",
        "  validation_loss = state_dict['validation_loss']\n",
        "  return (validation_accuracy, validation_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7maVKRpr4dfs"
      },
      "source": [
        "### Define function for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPpz-z2G4iEX"
      },
      "source": [
        "# Define the train function\n",
        "!pip3 install tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def TRAIN(model, train_loader, valid_loader, num_epochs, criterion, optimizer, device, save_path):\n",
        "  best_acc = 0.0\n",
        "  training_loss = []\n",
        "  validation_loss = []\n",
        "  validation_acc = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(train_loader):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      labels = labels.long()\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    training_loss.append(train_loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      for images, labels in tqdm(valid_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.long()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "      \n",
        "      valid_loss = running_loss / len(valid_loader)\n",
        "      validation_loss.append(valid_loss)\n",
        "      valid_accuracy = running_corrects / float(len(valid_loader.dataset))\n",
        "      valid_accuracy = valid_accuracy.cpu().numpy()\n",
        "      validation_acc.append(valid_accuracy)\n",
        "\n",
        "      print('Epoch [{}/{}], Training Loss: {:.4f}, Validation Loss: {:.4f}, Validation Accuracy: {:.4f}' \n",
        "              .format(epoch + 1, num_epochs, train_loss, valid_loss, valid_accuracy))\n",
        "      \n",
        "      if valid_accuracy > best_acc:\n",
        "        best_acc = valid_accuracy\n",
        "        save(save_path, model, optimizer, best_acc)\n",
        "                  \n",
        "  print('Finished Training')\n",
        "  print('The best acuracy is:', best_acc)\n",
        "  \n",
        "  # Plot the training and validation loss and validation accuracy curves\n",
        "  plt.figure(figsize = (10, 6))\n",
        "  plt.plot(range(1, num_epochs + 1), training_loss, color = 'green', label = 'training loss')\n",
        "  plt.plot(range(1, num_epochs + 1), validation_loss, color = 'blue', label = 'validation loss')\n",
        "  plt.plot(range(1, num_epochs + 1), validation_acc, color = 'red', label = 'validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Number of Epochs')\n",
        "  plt.ylabel('Average value of loss for each epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE6bgv6yv5p2"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEwMOS8V0WzX"
      },
      "source": [
        "### Get the Dataframe (modified 17:35)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEl66xu304LI"
      },
      "source": [
        "from xml.etree.ElementTree import parse\n",
        "import pandas as pd \n",
        "import os\n",
        "directory = 'gdrive/MyDrive/annotations'\n",
        "target_df = pd.DataFrame(columns=['imagename','xmin','ymin','xmax','ymax','target']) \n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    #get the file, in that directory\n",
        "    if filename.endswith(\".xml\"):\n",
        "      filename = os.path.join(directory, filename)\n",
        "      document = parse(filename)\n",
        "      for item in document.iterfind('object'):\n",
        "        imagename = document.findtext('filename')\n",
        "        target = []\n",
        "        target.append(item.findtext('name'))\n",
        "        #go to the bndbox tag, there will be different coordinates for people in the image\n",
        "        for x in item.iterfind('bndbox'):\n",
        "          xmin = []\n",
        "          xmax = []\n",
        "          ymin = []\n",
        "          ymax = []\n",
        "          xmax.append(x.findtext('xmax'))\n",
        "          xmin.append(x.findtext('xmin'))\n",
        "          ymin.append(x.findtext('ymin'))\n",
        "          ymax.append(x.findtext('ymax'))\n",
        "          # convert all the different attribute into one dataframe \n",
        "          df = pd.DataFrame({'imagename':imagename,'xmin': xmin , 'ymin':ymin, 'xmax':xmax,'ymax':ymax,'target':target})\n",
        "          #concat the dataframe\n",
        "          target_df = pd.concat([target_df,df],axis=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMVyvy1fikDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50c148ea-b08f-43ea-f1c1-1cde939c493e"
      },
      "source": [
        "print(target_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             imagename xmin ymin xmax ymax     target\n",
            "0  maksssksksss101.png   48  294  164  400  with_mask\n",
            "0   maksssksksss10.png   98  267  194  383  with_mask\n",
            "0  maksssksksss103.png   42   54   94  110  with_mask\n",
            "0  maksssksksss103.png  188   46  236  106  with_mask\n",
            "0  maksssksksss103.png  261   88  303  130  with_mask\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUHMkPv70VoO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "aa8d54bb-b4b7-43a1-dec2-5fbb2f977c9b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "with_mask = target_df[target_df.target == 'with_mask']\n",
        "without_mask = target_df[target_df.target == 'without_mask']\n",
        "incorrect_weared = target_df[target_df.target == 'mask_weared_incorrect']\n",
        "\n",
        "with_mask_train, with_mask_valid = train_test_split(with_mask, test_size = 0.2, random_state = 4211)\n",
        "without_mask_train, without_mask_valid = train_test_split(without_mask, test_size = 0.2, random_state = 4211)\n",
        "incorrect_weared_train, incorrect_weared_valid = train_test_split(incorrect_weared, test_size = 0.2, random_state = 4211)\n",
        "\n",
        "without_mask_train = without_mask_train.sample(len(with_mask_train), replace = True, random_state = 4211)\n",
        "incorrect_weared_train = incorrect_weared_train.sample(len(with_mask_train), replace = True, random_state = 4211)\n",
        "\n",
        "train_df = pd.concat([with_mask_train, without_mask_train, incorrect_weared_train], ignore_index=True)\n",
        "valid_df = pd.concat([with_mask_valid, without_mask_valid, incorrect_weared_valid], ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-45082046fc5c>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    print(train_df.class)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAECpBrhcNlI"
      },
      "source": [
        "### Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "of6A9vkgKjVh"
      },
      "source": [
        "# Transform\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((32, 32)), \n",
        "                  transforms.RandomHorizontalFlip(p = 0.5),\n",
        "                  transforms.ToTensor(),\n",
        "                  transforms.Normalize((0.5, 0.5, 0.5 ), (0.5, 0.5, 0.5))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PVB_n9vGMEw"
      },
      "source": [
        "# Create Dateset\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import linecache\n",
        "\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform):\n",
        "        self.data = dataframe \n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # First get the image\n",
        "        image_path = self.data.iloc[idx, 0]\n",
        "        image_path = os.path.join(self.image_dir, image_path)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        # Get the corresponding part of image\n",
        "        box = (int(self.data.iloc[idx, 1]), int(self.data.iloc[idx, 2]), int(self.data.iloc[idx, 3]), int(self.data.iloc[idx, 4]))\n",
        "        region = image.crop(box)\n",
        "        region = transform(region)\n",
        "\n",
        "        # Get the label\n",
        "        label = self.data.iloc[idx, 5]\n",
        "        label = label_to_num(label)\n",
        "\n",
        "        return region, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1cJtt9weu11",
        "outputId": "30852f09-1534-4e58-d880-88781c7a8f3d"
      },
      "source": [
        "train_set = FaceDataset(train_df, PATH + 'images', transform = transform)\n",
        "valid_set = FaceDataset(valid_df, PATH + 'images', transform = transform)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_iter = DataLoader(dataset = train_set, batch_size = 64, shuffle = True)\n",
        "valid_iter = DataLoader(dataset = valid_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4275, -0.2941, -0.0353,  ..., -0.5216, -0.5216, -0.5216],\n",
              "         [-0.3647, -0.2314,  0.0196,  ..., -0.5137, -0.5137, -0.5137],\n",
              "         [-0.2784, -0.1529,  0.0980,  ..., -0.5059, -0.5137, -0.5137],\n",
              "         ...,\n",
              "         [-0.5451, -0.5451, -0.5451,  ...,  0.8353,  0.8275,  0.8275],\n",
              "         [-0.5294, -0.5373, -0.5529,  ...,  0.8353,  0.8196,  0.8118],\n",
              "         [-0.5137, -0.5216, -0.5451,  ...,  0.8275,  0.8118,  0.8039]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2vRHoyNU5JO"
      },
      "source": [
        "# This is to show what's inside the train_iter\n",
        "for inputs, labels in train_iter:\n",
        "  print(inputs.size())\n",
        "  print(labels.size())\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym5h1M353yyE"
      },
      "source": [
        "## Basic Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCHb8_GeVeu2"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxnFiV2aymnA"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZU5KXAnTQo"
      },
      "source": [
        "class Basic_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Basic_Model, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 6, kernel_size = 5)  \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.norm1 = nn.BatchNorm2d(6) \n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,  kernel_size=5)  \n",
        "        self.norm2 = nn.BatchNorm2d(16) \n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, image): \n",
        "        image = self.norm1(self.pool(F.relu(self.conv1(image)))) #6, 14, 14 \n",
        "        image = self.norm2(self.pool(F.relu(self.conv2(image)))) #16, 5, 5\n",
        "        image = image.view(-1, 16 * 5 * 5)\n",
        "        image = F.relu(self.fc1(image))\n",
        "        image = F.relu(self.fc2(image))\n",
        "        image = self.fc3(image)\n",
        "        return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5Wz4TL8VuiM"
      },
      "source": [
        "### Model Training & Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsuD-RjaVwsD"
      },
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_basic = Basic_Model()\n",
        "model_basic.to(device)\n",
        "\n",
        "optimizer = Adam(model_basic.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "TRAIN(model_basic, train_iter, valid_iter, 15, criterion, optimizer, device, f'model_tutorial_6.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pawNvEqc1lih"
      },
      "source": [
        "## Training (Share)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFG4rJlw1gjU"
      },
      "source": [
        "# Define the train function\n",
        "# Define save function\n",
        "def save(path, model, optimizer, validation_accuracy, validation_loss):\n",
        "  save_path = path\n",
        "  state_dict = {'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'validation_accuracy': validation_accuracy,\n",
        "              'validation_loss': validation_loss}\n",
        "\n",
        "  torch.save(state_dict, save_path)\n",
        "  print(f'Model saved to {save_path}')\n",
        "\n",
        "# Define load function\n",
        "# Result returned is a tuple stroing (validation_accuracy, validation_loss)\n",
        "def load(path, model, optimizer):\n",
        "  load_path = path \n",
        "  state_dict = torch.load(load_path)\n",
        "  model.load_state_dict(state_dict['model_state_dict'])\n",
        "  optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "  validation_accuracy = state_dict['validation_accuracy']\n",
        "  validation_loss = state_dict['validation_loss']\n",
        "  return (validation_accuracy, validation_loss)\n",
        "  \n",
        "def TRAIN(model, train_loader, valid_loader, num_epochs, criterion, optimizer, device, save_path):\n",
        "  training_loss = []\n",
        "  training_acc = []\n",
        "  validation_loss = []\n",
        "  validation_acc = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    for images, labels in train_loader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "      labels = labels.long()\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      _, preds = torch.max(outputs.data, 1)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    training_loss.append(train_loss)\n",
        "    train_accuracy = running_corrects / float(len(train_loader.dataset))\n",
        "    training_acc.append(train_accuracy)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      model.eval()\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "      for images, labels in valid_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels.long()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "      \n",
        "      valid_loss = running_loss / len(valid_loader)\n",
        "      validation_loss.append(valid_loss)\n",
        "      x = running_corrects / float(len(valid_loader.dataset))\n",
        "      validation_acc.append(x)\n",
        "\n",
        "      print('Epoch [{}/{}], Training Loss: {:.4f}, Validation Loss: {:.4f}' \n",
        "              .format(epoch + 1, num_epochs, train_loss, valid_loss))\n",
        "                  \n",
        "  print('Finished Training')\n",
        "  \n",
        "  # Plot the training and validation loss and accuracy curves\n",
        "  plt.figure(figsize = (10, 6))\n",
        "  plt.plot(range(1, num_epochs + 1), training_loss, color = 'green', label = 'training loss')\n",
        "  plt.plot(range(1, num_epochs + 1), training_acc, color = 'yellow', label = 'validation loss')\n",
        "  plt.plot(range(1, num_epochs + 1), validation_loss, color = 'purple', label = 'training accuracy')\n",
        "  plt.plot(range(1, num_epochs + 1), validation_acc, color = 'red', label = 'validation accuracy')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Number of Epochs')\n",
        "  plt.ylabel('Average value of loss for each epoch')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vZ6k5vW2u5u"
      },
      "source": [
        "## Model (Share)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RK0g_sku2uCQ"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "    \n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1, stride=1)  \n",
        "        self.norm1 = nn.BatchNorm2d(32) \n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=1, stride=1)\n",
        "        self.norm2 = nn.BatchNorm2d(32) \n",
        "\n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=1)\n",
        "        self.norm3 = nn.BatchNorm2d(64) \n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1)\n",
        "        self.norm4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1)\n",
        "        self.norm5 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1)\n",
        "        self.norm6 = nn.BatchNorm2d(512)\n",
        "        self.AvgPool = nn.AvgPool2d(kernel_size=(16,16))\n",
        "\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(1*1*512, 512)\n",
        "        self.fc2 = nn.Linear(1*1*512, 3)\n",
        "    \n",
        "    \n",
        "    def forward(self, x1):\n",
        "        #  3, 32, 32\n",
        "        # out_dim = in_dim - kernel_size + 1  \n",
        "        x1 = F.relu(self.norm1(self.conv1(x1))) #6, 14, 14 \n",
        "        x1 = F.relu(self.norm2(self.conv2(x1)))\n",
        "        x1 = self.MaxPool(x1)\n",
        "        x1 = F.relu(self.norm3(self.conv3(x1)))\n",
        "        x1 = F.relu(self.norm4(self.conv4(x1)))\n",
        "        x1 = F.relu(self.norm5(self.conv5(x1)))\n",
        "        x1 = F.relu(self.norm6(self.conv6(x1)))\n",
        "        x1 = self.AvgPool(x1)\n",
        "\n",
        "        x1 = x1.view(-1, 1*1*512)\n",
        "\n",
        "        h3 = x1\n",
        "        \n",
        "        h3 = F.relu(self.fc1(h3))\n",
        "        h3 = self.drop(h3)\n",
        "\n",
        "        h3 = self.fc2(h3)\n",
        "        # h3 = self.sig(h3)\n",
        "        return h3\n",
        "    \n",
        "    \n",
        "    def aggregation(self, x1, x2):\n",
        "      \n",
        "      combined_x = abs(x1 - x2)\n",
        "      \n",
        "      return combined_x\n",
        "\n",
        "    def merge(self, x1, x2):\n",
        "      \n",
        "      return x1.append(x2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}